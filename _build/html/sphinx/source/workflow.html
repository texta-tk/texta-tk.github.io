
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="et">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Using TEXTA Toolkit &#8212; Toolkit 1.3 dokumentatsioon</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/translations.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Indeks" href="../../genindex.html" />
    <link rel="search" title="Otsing" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="using-texta-toolkit">
<h1>Using TEXTA Toolkit<a class="headerlink" href="#using-texta-toolkit" title="Püsiviit sellele pealkirjale">¶</a></h1>
<div class="section" id="first-steps">
<h2>First steps<a class="headerlink" href="#first-steps" title="Püsiviit sellele pealkirjale">¶</a></h2>
<div class="section" id="the-login-screen">
<h3>The login screen<a class="headerlink" href="#the-login-screen" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>After starting up TEXTA, as described in the <a class="reference internal" href="installation.html#running-texta"><span class="std std-ref">installation step</span></a>, the next intuitive thing is to start using it.
Since it is a web application, we have to navigate to the corresponding address in our browser
(e.g. <a class="reference external" href="http://localhost:8000/">http://localhost:8000/</a> if running locally). We are welcomed by a login page as depicted in Figure 1.</p>
<div class="figure" id="id1">
<span id="figure-1"></span><img alt="../../_images/01_welcome.png" src="../../_images/01_welcome.png" />
<p class="caption"><span class="caption-text">Figure 1. <em>Welcome screen</em></span></p>
<div class="legend">
<ol class="arabic simple">
<li>Login area</li>
<li>Create user</li>
</ol>
</div>
</div>
<p>Login page allows to login, as well as create a user.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<dl class="last docutils">
<dt>When starting up the TEXTA instance for the first time, it is crucial to create the superuser account (<a class="reference internal" href="installation.html#final-touches"><span class="std std-ref">installation’s final touches</span></a>).</dt>
<dd>The supersuser account is used to set up TEXTA and it’s features to all other users.</dd>
</dl>
</div>
</div>
<div class="section" id="after-the-login">
<h3>After the login<a class="headerlink" href="#after-the-login" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Once we have logged in with our superuser, we reach the home page, which looks much like the page before,
with the exception of a list of tools and some global settings.</p>
<div class="figure" id="id2">
<span id="figure-2"></span><img alt="../../_images/02_after_login.png" src="../../_images/02_after_login.png" />
<p class="caption"><span class="caption-text">Figure 2. <em>Home page</em></span></p>
</div>
<p>As we can see from the global settings panel, we don’t have any datasets nor language models.
Therefore, we need to do some setting up in „Administration“.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Restricted contains the superuser tools for managing users, datasets, language models and text classifiers.</p>
</div>
</div>
</div>
<div class="section" id="importing-data">
<h2>Importing data<a class="headerlink" href="#importing-data" title="Püsiviit sellele pealkirjale">¶</a></h2>
<p>To start analyzing data, we need some in the first place.  As the toolkit relies on Elasticsearch database, we could
insert data manually, while conforming to the rules and schema described
<a class="reference internal" href="elastic_schema.html#elastic-schema"><span class="std std-ref">here</span></a>.</p>
<p>However, as this might take a lot of work, the toolkit comes with a graphical data importing tool called „Dataset Importer“,
which can be found under „Restricted“ menu on the top.</p>
<div class="section" id="dataset-importer">
<h3>Dataset Importer<a class="headerlink" href="#dataset-importer" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Dataset Importer („importer“ from now on) is a tool which allows to insert data in many formats, preprocess it, and
finally store it in the underlying Elasticsearch database, so that it could be then used for analyzing using the other
tools the importer provides.</p>
<div class="figure" id="id3">
<img alt="../../_images/01_overview.png" src="../../_images/01_overview.png" />
<p class="caption"><span class="caption-text">Figure: Dataset Importer</span></p>
</div>
<p>We insert data with import jobs - requests for the server to process and store the provided documents.</p>
<div class="section" id="creating-a-new-import-job">
<h4>Creating a new import job<a class="headerlink" href="#creating-a-new-import-job" title="Püsiviit sellele pealkirjale">¶</a></h4>
<div class="section" id="selecting-formats">
<h5>Selecting formats<a class="headerlink" href="#selecting-formats" title="Püsiviit sellele pealkirjale">¶</a></h5>
<p>To create a new import job, we must first list all the formats that we have and from which we want to import. For that
we select all the applicable formats from the „Select all applicable formats“ drop-down menu. For example, let’s suppose
we want to import data from PDF and TXT and that they are in a ZIP archive.</p>
<div class="figure" id="id4">
<img alt="../../_images/02_selecting_formats.png" src="../../_images/02_selecting_formats.png" />
<p class="caption"><span class="caption-text">Figure: Selecting formats which we want to import to TEXTA Toolkit from our data source</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last"><strong>Simple documents</strong> store the content of the file to the field named „text“. Simple document <em>a.ext</em> can also be accompanied
in an archive by <em>a.meta.json</em> JSON file, which has other features, such as author, timestamp, or topic. All the
JSON file’s keys and values end up in the final dataset as columns and values.</p>
</div>
</div>
<div class="section" id="specifying-input-data-parameters">
<h5>Specifying input data parameters<a class="headerlink" href="#specifying-input-data-parameters" title="Püsiviit sellele pealkirjale">¶</a></h5>
<p>After we have selected the formats, the necessary fields which need filling will be displayed under „Input Data“ section.
We need to fill in additional information because importer has to know which data and from where to fetch. Also, some
formats need further instructions - e.g a password for a ZIP archive or an XPath query for an XML document.</p>
<p>For TXT, PDF, and ZIP files we currently only have to specify the source from where should the importer retrieve the data.
For regular files, the importer supports uploading a single file (could be an archive as well), downloading the file
from an URL, or loading from the server’s local file system.</p>
<div class="figure" id="id5">
<img alt="../../_images/03_specifying_input_data.png" src="../../_images/03_specifying_input_data.png" />
<p class="caption"><span class="caption-text">Figure: Specifying data source</span></p>
</div>
</div>
<div class="section" id="specifying-storage-options">
<h5>Specifying storage options<a class="headerlink" href="#specifying-storage-options" title="Püsiviit sellele pealkirjale">¶</a></h5>
<p>Once the importer knows where to get the data from - and in some instances, how - we now need to specify to where and how
to store the imported data. For that we need to fill in the fields under „TEXTA Dataset“ section.
We must name the dataset (will be used as both Elasticsearch index and mapping name).</p>
<dl class="docutils">
<dt>In addition, we can optionally</dt>
<dd><ul class="first last simple">
<li>specify a list of fields to be left untouched by Elasticsearch’es processors;</li>
<li>ask to keep the database synchronized with the data source, if possible, and</li>
<li>ask to overwrite an existing dataset, if the names collide.</li>
</ul>
</dd>
</dl>
<div class="figure" id="id6">
<img alt="../../_images/04_specifying_storage_options.png" src="../../_images/04_specifying_storage_options.png" />
<p class="caption"><span class="caption-text">Figure: Specifying storage options</span></p>
</div>
</div>
<div class="section" id="specifying-preprocessors">
<h5>Specifying preprocessors<a class="headerlink" href="#specifying-preprocessors" title="Püsiviit sellele pealkirjale">¶</a></h5>
<p>Finally, we can optionally specify the preprocessors we want to apply. Each preprocessor enhances the final dataset (data table) with
additional features (columns). To apply a preprocessor to the import job, select the desired preprocessor and fill in
the fields it requires. For example, if we want to lemmatize our documents and detect the language, we can use TEXTA’s
Multilingual Processor.</p>
<div class="figure" id="id7">
<img alt="../../_images/05_specifying_preprocessors.png" src="../../_images/05_specifying_preprocessors.png" />
<p class="caption"><span class="caption-text">Figure: Specifying preprocessors</span></p>
</div>
</div>
</div>
<div class="section" id="submitting-the-import-job">
<h4>Submitting the import job<a class="headerlink" href="#submitting-the-import-job" title="Püsiviit sellele pealkirjale">¶</a></h4>
<p>After we have filled in all the necessary fields, we are allowed to press „Import“ button.</p>
</div>
<div class="section" id="tracking-the-import-jobs">
<h4>Tracking the import jobs<a class="headerlink" href="#tracking-the-import-jobs" title="Püsiviit sellele pealkirjale">¶</a></h4>
<p>All the import jobs that have been completed or are still in progress are displayed in the „Import Jobs“ table. Here we
can see how far are the current import jobs and also which are the parameters and other details of all the started
import jobs.</p>
<div class="figure" id="id8">
<img alt="../../_images/06_tracking_import_jobs.png" src="../../_images/06_tracking_import_jobs.png" />
<p class="caption"><span class="caption-text">Figure: Tracking current and past import jobs</span></p>
</div>
<p>We can also remove an import job entry or see further details by clicking on the eye icon.</p>
<div class="figure" id="id9">
<img alt="../../_images/07_import_job_details.png" src="../../_images/07_import_job_details.png" />
<p class="caption"><span class="caption-text">Figure: Specific import job’s details</span></p>
</div>
<p>The parameters JSON content is the internal representation of the job and can mostly be used for debugging.</p>
</div>
</div>
</div>
<div class="section" id="administration-manage-users-and-datasets">
<h2>Administration: Manage Users and Datasets<a class="headerlink" href="#administration-manage-users-and-datasets" title="Püsiviit sellele pealkirjale">¶</a></h2>
<dl class="docutils">
<dt>The biggest bosses in TEXTA Toolkit are the superusers, whose privileges include:</dt>
<dd><ol class="first last arabic simple">
<li>Managing Users and their access rights (in Administration)</li>
<li>Managing datasets (in Administration)</li>
<li>Training language models (in Model Manager)</li>
<li>Training and applying text classifiers (in Classification Manager)</li>
</ol>
</dd>
</dl>
<p>Naturally, there can be more than one superuser.
New superusers can be created by either by promoting existing user to superusers in Administration or by using the command described in
<a class="reference internal" href="installation.html#final-touches"><span class="std std-ref">installation’s final touches</span></a>.</p>
<div class="section" id="managing-users-and-their-access-rights">
<h3>Managing users and their access rights<a class="headerlink" href="#managing-users-and-their-access-rights" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Users and their access to datasets can be configured in the „User Access Management“ panel in „Administration“:</p>
<div class="figure" id="id10">
<img alt="../../_images/03_user.png" src="../../_images/03_user.png" />
<p class="caption"><span class="caption-text">Figure 3. <em>Panel in Administration for managing users</em></span></p>
</div>
<p>Each new user will be created either as activated or deactivated, in which case a superuser has to manually activate each user by clicking „activate“.
By default, new users will be created as deactivated, but this can be changed in settings.py by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">USER_ISACTIVE_DEFAULT</span> <span class="o">=</span> <span class="bp">True</span>
</pre></div>
</div>
<p>User’s access to existing datasets can be managed by clicking on the username, which opens a modal:</p>
<div class="figure" id="id11">
<img alt="../../_images/04_user_datasets.png" src="../../_images/04_user_datasets.png" />
<p class="caption"><span class="caption-text">Figure 4. <em>The datasets can be moved between the two fields to determine user’s access to it</em></span></p>
</div>
</div>
<div class="section" id="managing-datasets">
<h3>Managing datasets<a class="headerlink" href="#managing-datasets" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Superusers can add datasets by selecting the according index and mapping in the „Dataset Management“ panel:</p>
<div class="figure" id="id12">
<img alt="../../_images/05_datasets.png" src="../../_images/05_datasets.png" />
<p class="caption"><span class="caption-text">Figure 4. <em>Adding a new dataset</em></span></p>
</div>
<p>Each new dataset can either be public or private. Public datasets are accessible for all users by default, but exceptions can be made in „User Management“ panel.
In contrast to public datasets, private datasets are closed to everyone (except the supersusers) by default. Again, individual access can be granted in „User Management“ panel.</p>
<p>Datasets can be closed and opened. Closed datasets are not listed to the users. Datasets can also be removed from TEXTA.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Removing dataset in TEXTA does not delete the actual dataset on the disk, but rather deletes the link between Elasticsearch index and TEXTA.</p>
</div>
</div>
<div class="section" id="training-language-models">
<h3>Training language models<a class="headerlink" href="#training-language-models" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>In order to successfully extract terminology from a dataset, one needs a language model. Language models can be trained
with „Model Manager“ application under „Terminology Management“.</p>
<div class="figure" id="id13">
<img alt="../../_images/05_model_manager.png" src="../../_images/05_model_manager.png" />
<p class="caption"><span class="caption-text">Figure 5. <em>Model Manager</em></span></p>
<div class="legend">
<ol class="arabic simple">
<li>New model parameters</li>
<li>Trained models</li>
</ol>
</div>
</div>
<p>To train a model, we need to specify the training data (by using the corresponding search).
By default, all exsisting documents in the given dataset are used.</p>
<p>The training process also requires a field in the given dataset to be used as input for the language model.</p>
<p>We can reduce the lexicon or data sparsity further by coding punctuation and numbers. This means that we replace all occurrences of
punctuation marks with a single token and numbers with another one. Replacing numbers makes often sense when training language models, as
different numerical values rarely add any semantical value.</p>
<p>Let’s train a new language model on our whole data. For that we use the default empty search.</p>
<div class="figure" id="id14">
<img alt="../../_images/05-1_model_parameters.png" src="../../_images/05-1_model_parameters.png" />
<p class="caption"><span class="caption-text">Figure 5.1. <em>Model parameters</em></span></p>
</div>
<p>After starting the model training task, we can see the progress. For progress upgrade, we have to refresh the page.</p>
<div class="figure" id="id15">
<img alt="../../_images/05-2_model_training_progress.png" src="../../_images/05-2_model_training_progress.png" />
<p class="caption"><span class="caption-text">Figure 5.2. <em>Model training progress</em></span></p>
</div>
<p>Once the training completes, we can see the following.</p>
<div class="figure" id="id16">
<img alt="../../_images/05-3_model_training_completed.png" src="../../_images/05-3_model_training_completed.png" />
<p class="caption"><span class="caption-text">Figure 5.3. <em>Training completed</em></span></p>
</div>
</div>
</div>
<div class="section" id="home-select-datasets-and-language-models">
<h2>Home: Select datasets and language models<a class="headerlink" href="#home-select-datasets-and-language-models" title="Püsiviit sellele pealkirjale">¶</a></h2>
<p>The Home application is where the users can select dataset and language model they are working with.
In order to update the changes, the user is required to press „Update settings“:</p>
<div class="figure">
<img alt="../../_images/02_settings.png" src="../../_images/02_settings.png" />
</div>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Datasets can be defined in the Administration panel.
Languagem models can be trained in the Model Manager application.</p>
</div>
</div>
<div class="section" id="searcher-explore-the-data">
<h2>Searcher: Explore the Data<a class="headerlink" href="#searcher-explore-the-data" title="Püsiviit sellele pealkirjale">¶</a></h2>
<p>The Searcher application is responsible for both creating the searches for other Toolkit’s other applications and browsing-summarizing the data.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">In order to use Searcher, at least one dataset must be defined in Administration application.</p>
</div>
<p>Searcher’s graphical interface consists of serveral important panels, which are depicted in figure 6.</p>
<div class="figure" id="id17">
<img alt="../../_images/06_corpus_tool.png" src="../../_images/06_corpus_tool.png" />
<p class="caption"><span class="caption-text">Figure 6. <em>Searcher’s first look</em></span></p>
<div class="legend">
<ol class="arabic simple">
<li>Current Search</li>
<li>Saved Searches</li>
<li>Aggregations</li>
<li>Results</li>
</ol>
</div>
</div>
<div class="section" id="creating-a-new-search">
<h3>Creating a new search<a class="headerlink" href="#creating-a-new-search" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Data browsing and summarization depend on searches. Search consists of a set of constraints on feature values. We can define our constraints on
the data using the „Current Search“ panel. Without saving the constraints, we are in a „test mode“, which means that we can use the search in
Searcher, but we cannot use the search in other tools. After saving the search, it is available also to other tools.</p>
<p>In order to add a constraint, we must first choose a field. After the field is selected, we can then specify which textual tokens should or
must occur in the interested document subset.</p>
<p>Suppose we are interested in finding all the documents which contains „bribery“ in Estonian. It makes sense to abuse lemmas
whenever possible to account for inflection.</p>
<div class="figure" id="id18">
<img alt="../../_images/06-1_bribe_search_constraints.png" src="../../_images/06-1_bribe_search_constraints.png" />
<p class="caption"><span class="caption-text">Figure 6.1. <em>„Bribe“ search constraints</em></span></p>
</div>
<p>Figure 6.1 shows how we have defined that we want to find all the documents which contain either „pistis“ <em>or</em> „altkäemaks“
(„bribery“ in Estonian). „Match“ and „Match phrase“ mean that we want to find exact matches, whereas „Match phrase prefix“ matches prefixes
(meaning suffixes may differ).</p>
<p>Should we be interested in more detailed searches, we can add more constraints like the previous one.</p>
<p>After we have come up with a suitable search, we can save it for later uses.</p>
</div>
<div class="section" id="browsing-data">
<h3>Browsing data<a class="headerlink" href="#browsing-data" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>If we click on „Search“ button, we will see the matching data in a tabular form, where layered features share feature name’s prefix, and
matches are highlighted.</p>
<div class="figure" id="id19">
<img alt="../../_images/06-2_bribe_results.png" src="../../_images/06-2_bribe_results.png" />
<p class="caption"><span class="caption-text">Figure 6.2. <em>Bribe search results</em></span></p>
</div>
<p>We can see some basic statistics and if there are too many features, we can hide them by clicking on their green names.</p>
</div>
<div class="section" id="exporting-data">
<h3>Exporting data<a class="headerlink" href="#exporting-data" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Sometimes we want to work with a subset of data in some other application or external calculation. For example, we might want to train a
classifier on enriched sample. To get the enriched sample (in which some classes or tokens are over-represented), we can apply the search
constraints to retrieve the data and then use query result actions, such as <em>export</em>.</p>
<div class="figure" id="id20">
<img alt="../../_images/06-3_export_panel.png" src="../../_images/06-3_export_panel.png" />
<p class="caption"><span class="caption-text">Figure 6.3. <em>Export panel</em></span></p>
</div>
<p>Export panel allows to specify, how many rows and which features are we interested in. Exported data is in CSV format.</p>
</div>
<div class="section" id="deleting-data">
<h3>Deleting data<a class="headerlink" href="#deleting-data" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>The second action on search results is deletion - if we detect some malformed data or are simply not interested in some subset, we can remove it
permanently from the Elasticsearch.</p>
</div>
<div class="section" id="using-saved-searches">
<h3>Using saved searches<a class="headerlink" href="#using-saved-searches" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Searches can be saved. If we save our „bribery“ search under „bribery“, we can see it being listed in „Saved Searches“ panel.</p>
<div class="figure" id="id21">
<img alt="../../_images/06-4_saved_search.png" src="../../_images/06-4_saved_search.png" />
<p class="caption"><span class="caption-text">Figure 6.4. <em>Saved searches</em></span></p>
</div>
<p>Now, whenever we check it, we can use it to browse data or apply in summarization.</p>
</div>
<div class="section" id="summarizing-data">
<h3>Summarizing data<a class="headerlink" href="#summarizing-data" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>As fun as browsing through the data is, it is not always enough. Sometimes we want to get an overview of our data, such as topics over time or
word distributions. Searcher allows to do all of that and more through the „Aggregations“ panel.</p>
<p>Aggregations have two components - data and features it aggregates over. Selecting a search determines the sample we get our data from. By defining a feature, we can group by that feature and get
category counts. For example, lets assume we are interested in seeing how are the top words distributed in our sample data defined by our
„bribery“ search. By requesting aggregation as shown on figure 6.5, we get the result on the same figure.</p>
<div class="figure" id="id22">
<img alt="../../_images/06-5_simple_aggregation.png" src="../../_images/06-5_simple_aggregation.png" />
<p class="caption"><span class="caption-text">Figure 6.5. <em>Simple aggregation</em></span></p>
</div>
<p>From the results we can see raw word distributions for both checked „bribery“ search and „Current Search“ (which doesn’t have any constraints,
a.k.a sample is all the data we have). Since we queried raw count, many common words overlap. We can change „Sort by“ setting to significance
in order to get uncommon over-represented words for that specific sample dataset.</p>
<div class="figure" id="id23">
<img alt="../../_images/06-6_significance_aggregation.png" src="../../_images/06-6_significance_aggregation.png" />
<p class="caption"><span class="caption-text">Figure 6.6. <em>Aggregation sorted by significance</em></span></p>
</div>
<p>In figure 6.6 we can see that now the words are much more specific to the „bribery“ dataset. „Current Search“ has no results, because it is
used as prior.</p>
</div>
</div>
<div class="section" id="extracting-terminology">
<h2>Extracting Terminology<a class="headerlink" href="#extracting-terminology" title="Püsiviit sellele pealkirjale">¶</a></h2>
<p>In order to learn more about the dataset, it is useful to know the domain terminology.
TEXTA Toolkit’s terminology extraction tools support the user through the process of creating lexicons,
grouping them into concepts and mining for multiword expressions.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Extracting Terminology requires a language model, which can be trained by superusers in Model Manager.</p>
</div>
<div class="section" id="creating-lexicons">
<h3>Creating lexicons<a class="headerlink" href="#creating-lexicons" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>We can start creating topic-related lexicons. From toolbar we can find „Base Lexicon Miner“ under „Terminology Management“.</p>
<p>Let’s create a lexicon that contains verbs accompanied with „bribery“.</p>
<div class="figure" id="id24">
<img alt="../../_images/07_creating_verb_lexicon.png" src="../../_images/07_creating_verb_lexicon.png" />
<p class="caption"><span class="caption-text">Figure 7. <em>Creating lexicon of bribery verbs</em></span></p>
</div>
<p>After clicking on the newly created lexicon, we have to provide some seed words.</p>
<div class="figure" id="id25">
<img alt="../../_images/07-1_lexicon_seed_words.png" src="../../_images/07-1_lexicon_seed_words.png" />
<p class="caption"><span class="caption-text">Figure 7.1. <em>Providing seed words</em></span></p>
</div>
<p>The process of creating (or expanding) the lexicon is iterative. We keep asking for suggestions and from those we have to pick the ones that
make sense to us. We keep asking for suggestions until we get no more meaningful responses. Then we should either change to some approach with
„preclustering“ in it or end the process, as the training data didn’t give us more.</p>
<p>The first batch of suggested words are shown in figure 7.2.</p>
<div class="figure" id="id26">
<img alt="../../_images/07-2_first_suggestion_batch.png" src="../../_images/07-2_first_suggestion_batch.png" />
<p class="caption"><span class="caption-text">Figure 7.2. <em>First suggestion batch</em></span></p>
</div>
<p>The first suggested word - „kahtlustama“ - makes sense, while the others not so much. To add it to the lexicon, we simply have to click on it. In the next batches
we also get „seostama“ and „avastama“. However, the yield is not as good as we hoped for. The reason behind this is that the training data is
too small. We had less than 5000 documents, most of which didn’t even contain the relevant words and therefore the model had difficulties during
the training phase.</p>
</div>
<div class="section" id="creating-concepts">
<h3>Creating concepts<a class="headerlink" href="#creating-concepts" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Once we have saved the lexicons we are interested in, the next step would be to group parts of them into concepts. A lexicon may contain
somewhat similar words which still differ from one another in some important aspects. Concepts are created with „Conceptualiser“ under
„Terminology Management“. It takes lexicons as input and outputs concepts, which user defines using the graphical tool. Words are displayed on
scatter plot and user can group them using selection box or merge one by one using the <em>enter</em> key.</p>
<p>Word coordinates in scatter plot are derived by applying dimension reduction on high dimension word vectors. Word vectors are relying on
distributional semantics, meaning that words with similar context are similar and have in our case similar vectors - or are close to each other
in 2-dimensional space.</p>
<p>One of several dimension reduction methods can be chosen, but they give approximately the same results.</p>
<p>In our scenario, we have small and homogeneous lexicons and therefore each lexicon forms just one concept.</p>
<p>After checking the lexicons and plotting them, we get to the state depicted in figure 8.</p>
<div class="figure" id="id27">
<img alt="../../_images/08_conceptualiser_initial_plot.png" src="../../_images/08_conceptualiser_initial_plot.png" />
<p class="caption"><span class="caption-text">Figure 8. <em>Conceptualiser’s initial plot</em></span></p>
</div>
<p>We can visually detect that two more or less coherent clusters have formed and „avastama“ is an outlier. It was also the last word suggested by
<em>Lexicon Miner</em>. For that reason we leave that word out from our concepts.</p>
<div class="figure" id="id28">
<img alt="../../_images/08-1_grouped_concepts.png" src="../../_images/08-1_grouped_concepts.png" />
<p class="caption"><span class="caption-text">Figure 8.1. <em>After grouping the words into concepts</em></span></p>
</div>
<p>Now that we have found the concepts, we can commit the changes to save them.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Concepts can be used in <em>Searcher</em> by prepending an „&#64;“-sign. So we don’t have to list words one by one. They can also be used in the same
manner in <em>Grammar tool</em>.</p>
</div>
</div>
<div class="section" id="mining-multi-word-expressions">
<h3>Mining multi-word expressions<a class="headerlink" href="#mining-multi-word-expressions" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Mining multi-word expressions is a way to find actually used phrases. We approched the problem bottom-up. First we defined the individual tokens
and now we try to find which of them are located nearby or side-by-side. Ideally, phrases should be found using the words with inflection data,
but since our dataset is small, we have to make it with lemmas and low frequency threshold.</p>
<p>Mining task requires parameters - much like training language models. In figure 9 we can see the parameters we can use.</p>
<div class="figure" id="id29">
<img alt="../../_images/09_mwe_parameters.png" src="../../_images/09_mwe_parameters.png" />
<p class="caption"><span class="caption-text">Figure 9. <em>Multi-word expression mining parameters</em></span></p>
</div>
<p>We have to define the feature or field, which should be the same we trained our language models on for mining lexicons. Expression lengths
determine the output phrase lengths (or combination lengths, which are searched for). Phrase occurrencies below frequency threshold are ignored
and slop determines, how far apart can the words be from one another. Finally, we have to specify the lexicons used. Since we want to find
bribery phrases which contain both noun and accusing verb, we check both lexicons.</p>
<p>Because the data and lexicons are small, the task completes instantly.</p>
<div class="figure" id="id30">
<img alt="../../_images/09-1_mwe_progress.png" src="../../_images/09-1_mwe_progress.png" />
<p class="caption"><span class="caption-text">Figure 9.1. <em>Multi-word expression task progress</em></span></p>
</div>
<p>By looking at the results, we can see that there are 9 different patterns (denoted by „Terms“ feature) containing „süüdistama“ and „altkäemaks“
concepts‘ lemmas which are frequent enough to catch our interest.</p>
<div class="figure" id="id31">
<img alt="../../_images/09-2_mwe_results.png" src="../../_images/09-2_mwe_results.png" />
<p class="caption"><span class="caption-text">Figure 9.2. <em>Multi-word expression results</em></span></p>
</div>
<p>We can expand the result by clicking on the „plus“-sign under „Accepted“ feature to see which patterns actually existed and with which
frequency.</p>
<div class="figure" id="id32">
<img alt="../../_images/09-3_expanded_results.png" src="../../_images/09-3_expanded_results.png" />
<p class="caption"><span class="caption-text">Figure 9.3. <em>Expanded results</em></span></p>
</div>
<p>The expanded results show how some patterns are much more common in real use of language.</p>
<p>We can approve specific patterns to turn them into a concept containing multi-word expressions and therefore use the more complicated structures
in other tools, such as in the <em>Searcher</em>.</p>
</div>
</div>
<div class="section" id="grammar-miner-extract-information">
<h2>Grammar Miner: Extract Information<a class="headerlink" href="#grammar-miner-extract-information" title="Püsiviit sellele pealkirjale">¶</a></h2>
<p>TEXTA comes with an interactive grammar building tool <em>Grammar Miner</em>. „Grammars“ are rule-based formulas which allow to match specific
content using exact matching, context, and logical operators. The simplest grammar can just match a fixed word, for example „bribe“, or be
a regular expression, while more complicated ones may cover whole phrase and sentence structures.</p>
<p>We build grammars from top to bottom using a graphical tree building tool <em>jstree</em>. Once a suitable grammar expression in the form of a tree is
created, we can test it on a data sample and see, which documents matched and how and which did not. <em>Grammar Miner</em> is under <em>Terminology
Management</em> tools.</p>
<div class="figure" id="id33">
<img alt="../../_images/10_grammar_miner.png" src="../../_images/10_grammar_miner.png" />
<p class="caption"><span class="caption-text">Figure 10. <em>Grammar Miner’s first look</em></span></p>
<div class="legend">
<ol class="arabic simple">
<li>View tab</li>
<li>Grammar building area</li>
<li>Grammar component details</li>
</ol>
</div>
</div>
<p>Building a new grammar begins with assigning an operation to the root node.</p>
<div class="figure" id="id34">
<img alt="../../_images/10-1_root_node_operation.png" src="../../_images/10-1_root_node_operation.png" />
<p class="caption"><span class="caption-text">Figure 10.1. <em>Assigning operation to aggregative node</em></span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Whenever making changes to a node, make sure to click on „Change“ button.</p>
</div>
<div class="section" id="node-types">
<h3>Node types<a class="headerlink" href="#node-types" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Each node has an icon, indicating which type of node it is. Nodes can be either <strong>terminal</strong> (regular expression, exact match) or
<strong>aggregative</strong> (logical or sequential operations).
Logical operations are intersection and union, where intersection needs all of its child expressions to
match, but union just one.</p>
<p>Sequential operations are concatenation and gap. Concatenation requires matches to reside side by side. For example, when we have a
concatenation of „took“, „the“, and „bribe“, the concatenation matches only documents in which there are substrings „took the bribe“. The gap
on the other hand can have matches with some distance from one another, defined by <em>slop</em> parameter. If we were looking for „took“ and „bribe“,
gap with <em>slop</em> of at least 1, it would match the documents which have „took the bribe“ in them.</p>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">Logical operations don’t take match order into account, whereas sequential operations do.</p>
</div>
<p>For a better overview, nodes with different operations have different icons.</p>
<table border="1" class="docutils">
<colgroup>
<col width="42%" />
<col width="58%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Icon</th>
<th class="head">Operation</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><img alt="na_icon" src="../../_images/na.ico" /></td>
<td>Not assigned</td>
</tr>
<tr class="row-odd"><td><img alt="exact_icon" src="../../_images/exact.ico" /></td>
<td>Exact match</td>
</tr>
<tr class="row-even"><td><img alt="regex_icon" src="../../_images/regex.ico" /></td>
<td>Regular expression</td>
</tr>
<tr class="row-odd"><td><img alt="and_icon" src="../../_images/and.ico" /></td>
<td>Intersection</td>
</tr>
<tr class="row-even"><td><img alt="or_icon" src="../../_images/or.ico" /></td>
<td>Union</td>
</tr>
<tr class="row-odd"><td><img alt="concat_icon" src="../../_images/concat.ico" /></td>
<td>Concatenation</td>
</tr>
<tr class="row-even"><td><img alt="gap_icon" src="../../_images/gap.ico" /></td>
<td>Gap</td>
</tr>
</tbody>
</table>
<p>Root node is always aggregative. Aggregative nodes can have both terminal and aggregative nodes as children. Terminal nodes can’t have any
children.</p>
</div>
<div class="section" id="adding-a-child">
<h3>Adding a child<a class="headerlink" href="#adding-a-child" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Aggregative nodes can (must) have child nodes (at least one). A child can be added by opening context menu with right click on the appropriate
node. This allows to add either a <em>basic</em> (terminal) or <em>aggregation</em> node.</p>
<div class="figure" id="id35">
<img alt="../../_images/10-2_node_context_menu.png" src="../../_images/10-2_node_context_menu.png" />
<p class="caption"><span class="caption-text">Figure 10.2. <em>Context menu for adding child nodes</em></span></p>
</div>
<p>After adding a terminal node and clicking on it, we can edit the details in opened „Component Details“ panel.</p>
<div class="figure" id="id36">
<img alt="../../_images/10-3_basic_component_details.png" src="../../_images/10-3_basic_component_details.png" />
<p class="caption"><span class="caption-text">Figure 10.3. <em>Specifying terminal node details</em></span></p>
</div>
<p>We can choose either „Exact match“ or „Regular Expression“ for <em>Type</em> and one of the features or feature-layer combinations for the <em>Layer</em>.
Content is a list of words on separate lines for an <em>Exact match</em> or a one-liner regular expression for <em>Regular Expression</em>.</p>
</div>
<div class="section" id="testing-grammar">
<h3>Testing grammar<a class="headerlink" href="#testing-grammar" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>We can either test the whole grammar tree by clicking on the „Test whole tree“ button or a subtree by clicking on „Test“ in the appropriate
node’s context menu.</p>
<p>Suppose we are interested in finding (and later labeling) the documents which talk about bribery accusations. We have already found out that
the most frequent pattern in the corresponding documents contain the lemmas „süüdistama“ and „altkäemaks“ in that order with the slop of 3.
Let’s create the grammar tree.</p>
<div class="figure" id="id37">
<img alt="../../_images/10-4_bribery_grammar.png" src="../../_images/10-4_bribery_grammar.png" />
<p class="caption"><span class="caption-text">Figure 10.4. <em>Simple bribery grammar</em></span></p>
</div>
<p>So far we have been on the „Grammar“ tab. If we test our grammar by clicking on „Test whole tree“, for example, we are taken to the „Test“ tab
and all the positive matches (documents which our grammar matched) are displayed along with the match highlights.</p>
<div class="figure" id="id38">
<img alt="../../_images/10-5_bribery_positive_results.png" src="../../_images/10-5_bribery_positive_results.png" />
<p class="caption"><span class="caption-text">Figure 10.5. <em>Positive results when testing</em></span></p>
</div>
<p>When navigating to „Unmatched documents“ subtab, we can see the documents which didn’t match our grammar.</p>
<div class="figure" id="id39">
<img alt="../../_images/10-6_bribery_negative_results.png" src="../../_images/10-6_bribery_negative_results.png" />
<p class="caption"><span class="caption-text">Figure 10.6. <em>Negative results when testing</em></span></p>
</div>
<p>We can see that all our highlighted words are „süüdistama“ and „altkäemaks“, where they are no more than 3 tokens apart - just as our grammar
required. Also, all the features which are displayed in the result table occur in the grammar, except for <em>id</em>.</p>
<p>By default, „Full search“ is used. We can change it to our „bribery“, navigate back to „Grammar“ tab and test the grammar again to test our
grammar on more relevant dataset.</p>
</div>
<div class="section" id="saving-and-loading-grammar">
<h3>Saving and loading grammar<a class="headerlink" href="#saving-and-loading-grammar" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>Grammar tree can be stored for later use by clicking „Save“. Root node’s label is used for the name.</p>
<p>Whenever we want to reuse a saved grammar, we can simply select it from drop-down menu and press „Load“.</p>
</div>
<div class="section" id="deleting-grammar">
<h3>Deleting grammar<a class="headerlink" href="#deleting-grammar" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>We can delete grammar trees by selecting the appropriate grammar from the drop-down menu and clicking on „Delete“.</p>
</div>
</div>
<div class="section" id="classificaton-manager-tag-the-texts">
<h2>Classificaton Manager: Tag the Texts<a class="headerlink" href="#classificaton-manager-tag-the-texts" title="Püsiviit sellele pealkirjale">¶</a></h2>
<p>When we have a set of documents in our dataset that we know are somehow important, we can build a text classifier to automatically detect such documents in the future.
In order to complete such a task, we can use the „Classification Manager“ application.</p>
<div class="section" id="training-a-classificaton-model">
<h3>Training a classificaton model<a class="headerlink" href="#training-a-classificaton-model" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>In order to train a model, we are required to define some mandatory parameters:</p>
<blockquote>
<div><ol class="arabic simple">
<li>A search to define the set of documents used to train the model (positive documents).</li>
<li>The field describes the field of the document used to build the classification model.</li>
<li>The name for the class or „tag“, which is later user to tag the documents.</li>
</ol>
</div></blockquote>
<p>By setting these three, we can now train a classifier. However, we can also fine-tune the classifier by changing additional parameters such as
feature extraction, dimensionality reduction and classifier model.</p>
<div class="figure" id="id40">
<img alt="../../_images/11-1_new_model.png" src="../../_images/11-1_new_model.png" />
<p class="caption"><span class="caption-text">Figure 11.1. <em>Choosing parameters for the classification model</em></span></p>
</div>
<p>Trained models are shown in the „Classification Models“ panel, where each epoch is equipped with precision and recall and some information about the classifier model.</p>
<div class="figure" id="id41">
<img alt="../../_images/11-2_trained_models.png" src="../../_images/11-2_trained_models.png" />
<p class="caption"><span class="caption-text">Figure 11.2. <em>Trained models</em></span></p>
</div>
</div>
<div class="section" id="tagging-the-dataset-with-the-model">
<h3>Tagging the dataset with the model<a class="headerlink" href="#tagging-the-dataset-with-the-model" title="Püsiviit sellele pealkirjale">¶</a></h3>
<p>By clicking „Apply“ in „Classification Models“ panel, user can apply the classifier on selected documents:</p>
<div class="figure" id="id42">
<img alt="../../_images/11-3_apply_model.png" src="../../_images/11-3_apply_model.png" />
<p class="caption"><span class="caption-text">Figure 11.2. <em>Select search to define the dataset to be tagged with the selected classifier</em></span></p>
</div>
<p>After pressing „Apply the Tagger“, a tagging job will start and it’s results will be shown in the table when completed.</p>
<div class="figure" id="id43">
<img alt="../../_images/11-4_applied_models.png" src="../../_images/11-4_applied_models.png" />
<p class="caption"><span class="caption-text">Figure 11.2. <em>Applied classification models</em></span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Märkus</p>
<p class="last">If the dataset contains many documents, the tagging process can be expected to take a few minutes.</p>
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Toolkit</a></h1>








<h3>Navigatsioon</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Kiirotsing</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Otsi" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Silver Traat.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.0.0+/7e143a0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/sphinx/source/workflow.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>